# Motion Transformer (MTR): https://arxiv.org/abs/2209.13508
# Published at NeurIPS 2022
# Written by Shaoshuai Shi
# All Rights Reserved


import numpy as np
import os
import jittor
import jittor.nn as nn
from .context_encoder import build_context_encoder
from .motion_decoder import build_motion_decoder

jittor.flags.use_cuda = 1


class MotionTransformer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.model_cfg = config

        self.context_encoder = build_context_encoder(self.model_cfg.CONTEXT_ENCODER)
        self.motion_decoder = build_motion_decoder(
            in_channels=self.context_encoder.num_out_channels,
            config=self.model_cfg.MOTION_DECODER,
        )

    def execute(self, batch_dict):
        batch_dict = self.context_encoder(batch_dict)
        batch_dict = self.motion_decoder(batch_dict)

        if self.training:
            loss, tb_dict, disp_dict = self.get_loss()

            tb_dict.update({"loss": loss.item()})
            disp_dict.update({"loss": loss.item()})
            return loss, tb_dict, disp_dict

        return batch_dict

    def get_loss(self):
        loss, tb_dict, disp_dict = self.motion_decoder.get_loss()

        return loss, tb_dict, disp_dict

    def load_params_with_optimizer(
        self, filename, to_cpu=False, optimizer=None, logger=None
    ):
        if not os.path.isfile(filename):
            raise FileNotFoundError

        logger.info(
            "==> Loading parameters from checkpoint %s to %s"
            % (filename, "CPU" if to_cpu else "GPU")
        )
        loc_type = "cpu" if to_cpu else None
        checkpoint = jittor.load(filename)
        epoch = checkpoint.get("epoch", -1)
        it = checkpoint.get("it", 0.0)

        self.load_state_dict(checkpoint["model_state"])

        if optimizer is not None:
            logger.info(
                "==> Loading optimizer parameters from checkpoint %s to %s"
                % (filename, "CPU" if to_cpu else "GPU")
            )
            optimizer.load_state_dict(checkpoint["optimizer_state"])

        if "version" in checkpoint:
            print("==> Checkpoint trained from version: %s" % checkpoint["version"])
        # logger.info('==> Done')
        logger.info(
            "==> Done (loaded %d/%d)"
            % (len(checkpoint["model_state"]), len(checkpoint["model_state"]))
        )

        return it, epoch

    def load_params_from_file(self, filename, logger, to_cpu=False):
        if not os.path.isfile(filename):
            raise FileNotFoundError

        logger.info(
            "==> Loading parameters from checkpoint %s to %s"
            % (filename, "CPU" if to_cpu else "GPU")
        )
        loc_type = "cpu" if to_cpu else None
        checkpoint = jittor.load(filename)
        model_state_disk = checkpoint["model_state"]

        version = checkpoint.get("version", None)
        if version is not None:
            logger.info("==> Checkpoint trained from version: %s" % version)

        logger.info(f"The number of disk ckpt keys: {len(model_state_disk)}")
        model_state = self.state_dict()
        model_state_disk_filter = {}
        for key, val in model_state_disk.items():
            if (
                key in model_state
                and model_state_disk[key].shape == model_state[key].shape
            ):
                model_state_disk_filter[key] = val
            else:
                if key not in model_state:
                    print(
                        f"Ignore key in disk (not found in model): {key}, shape={val.shape}"
                    )
                else:
                    print(
                        f"Ignore key in disk (shape does not match): {key}, load_shape={val.shape}, model_shape={model_state[key].shape}"
                    )

        model_state_disk = model_state_disk_filter

        # missing_keys, unexpected_keys = self.load_state_dict(
        #     model_state_disk, strict=False
        # )
        # missing_keys, unexpected_keys = self.load_state_dict(model_state_disk)

        # logger.info(f"Missing keys: {missing_keys}")
        # logger.info(f"The number of missing keys: {len(missing_keys)}")
        # logger.info(f"The number of unexpected keys: {len(unexpected_keys)}")
        self.load_state_dict(model_state_disk)
        logger.info("==> Done (total keys %d)" % (len(model_state)))

        epoch = checkpoint.get("epoch", -1)
        it = checkpoint.get("it", 0.0)

        return it, epoch
