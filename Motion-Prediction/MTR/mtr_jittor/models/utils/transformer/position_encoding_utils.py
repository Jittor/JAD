# Motion Transformer (MTR): https://arxiv.org/abs/2209.13508
# Published at NeurIPS 2022
# Modified by Shaoshuai Shi
# All Rights Reserved


import jittor
import math


def gen_sineembed_for_position(pos_tensor, hidden_dim=256):
    """Mostly copy-paste from https://github.com/IDEA-opensource/DAB-DETR/"""
    # n_query, bs, _ = pos_tensor.size()
    # sineembed_tensor = torch.zeros(n_query, bs, 256)
    half_hidden_dim = hidden_dim // 2
    scale = 2 * math.pi
    dim_t = jittor.arange(half_hidden_dim, dtype=jittor.float32)
    dim_t = 10000 ** (2 * (dim_t // 2) / half_hidden_dim)
    x_embed = pos_tensor[:, :, 0] * scale
    y_embed = pos_tensor[:, :, 1] * scale
    pos_x = x_embed[:, :, None] / dim_t
    pos_y = y_embed[:, :, None] / dim_t
    pos_x = jittor.stack(
        [pos_x[:, :, 0::2].sin(), pos_x[:, :, 1::2].cos()], dim=3
    ).flatten(2)
    pos_y = jittor.stack(
        [pos_y[:, :, 0::2].sin(), pos_y[:, :, 1::2].cos()], dim=3
    ).flatten(2)
    if pos_tensor.size(-1) == 2:
        pos = jittor.concat((pos_y, pos_x), dim=2)
    elif pos_tensor.size(-1) == 4:
        w_embed = pos_tensor[:, :, 2] * scale
        pos_w = w_embed[:, :, None] / dim_t
        pos_w = jittor.stack(
            [pos_w[:, :, 0::2].sin(), pos_w[:, :, 1::2].cos()], dim=3
        ).flatten(2)

        h_embed = pos_tensor[:, :, 3] * scale
        pos_h = h_embed[:, :, None] / dim_t
        pos_h = jittor.stack(
            [pos_h[:, :, 0::2].sin(), pos_h[:, :, 1::2].cos()], dim=3
        ).flatten(2)

        pos = jittor.contiguous((pos_y, pos_x, pos_w, pos_h), dim=2)
    else:
        raise ValueError("Unknown pos_tensor shape(-1):{}".format(pos_tensor.size(-1)))
    return pos
